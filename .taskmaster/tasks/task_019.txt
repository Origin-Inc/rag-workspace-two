# Task ID: 19
# Title: Implement Real-time Indexing Pipeline
# Status: done
# Dependencies: 6
# Priority: high
# Description: Build automatic content indexing with PostgreSQL triggers, Supabase Realtime subscriptions for instant updates, debounced batch processing, incremental index updates, and background re-indexing jobs for optimal search freshness without performance impact
# Details:
1. Create PostgreSQL trigger-based indexing system:
```sql
-- Create indexing queue table
CREATE TABLE indexing_queue (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  entity_type TEXT NOT NULL, -- 'page', 'block', 'document'
  entity_id UUID NOT NULL,
  operation TEXT NOT NULL, -- 'insert', 'update', 'delete'
  priority INTEGER DEFAULT 0,
  retry_count INTEGER DEFAULT 0,
  status TEXT DEFAULT 'pending', -- 'pending', 'processing', 'completed', 'failed'
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  processed_at TIMESTAMP WITH TIME ZONE
);

-- Create triggers for content changes
CREATE OR REPLACE FUNCTION queue_content_for_indexing()
RETURNS TRIGGER AS $$
BEGIN
  INSERT INTO indexing_queue (entity_type, entity_id, operation)
  VALUES (
    TG_TABLE_NAME,
    CASE
      WHEN TG_OP = 'DELETE' THEN OLD.id
      ELSE NEW.id
    END,
    TG_OP
  );
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Attach triggers to relevant tables
CREATE TRIGGER index_pages_changes
AFTER INSERT OR UPDATE OR DELETE ON pages
FOR EACH ROW EXECUTE FUNCTION queue_content_for_indexing();

CREATE TRIGGER index_blocks_changes
AFTER INSERT OR UPDATE OR DELETE ON blocks
FOR EACH ROW EXECUTE FUNCTION queue_content_for_indexing();
```

2. Implement Supabase Realtime subscription for instant updates:
```typescript
// services/indexing/realtime-indexer.ts
export class RealtimeIndexer {
  private channel: RealtimeChannel;
  private indexingBuffer: Map<string, IndexingTask> = new Map();
  private debounceTimer: NodeJS.Timeout | null = null;
  
  async initialize() {
    this.channel = supabase
      .channel('indexing-updates')
      .on(
        'postgres_changes',
        {
          event: '*',
          schema: 'public',
          table: 'indexing_queue',
          filter: 'status=eq.pending'
        },
        (payload) => this.handleIndexingEvent(payload)
      )
      .subscribe();
  }
  
  private handleIndexingEvent(payload: any) {
    const task: IndexingTask = {
      id: payload.new.id,
      entityType: payload.new.entity_type,
      entityId: payload.new.entity_id,
      operation: payload.new.operation,
      priority: payload.new.priority
    };
    
    // Buffer updates for batch processing
    this.indexingBuffer.set(task.entityId, task);
    this.scheduleBatchProcessing();
  }
  
  private scheduleBatchProcessing() {
    if (this.debounceTimer) {
      clearTimeout(this.debounceTimer);
    }
    
    this.debounceTimer = setTimeout(() => {
      this.processBatch();
    }, 500); // 500ms debounce
  }
}
```

3. Create batch processing system with intelligent debouncing:
```typescript
// services/indexing/batch-processor.ts
export class BatchIndexProcessor {
  private readonly BATCH_SIZE = 100;
  private readonly MAX_CONCURRENT = 5;
  
  async processBatch() {
    const tasks = await this.getTopPriorityTasks(this.BATCH_SIZE);
    
    // Group by entity type for efficient processing
    const grouped = this.groupTasksByType(tasks);
    
    // Process in parallel with concurrency limit
    await pLimit(this.MAX_CONCURRENT, Object.entries(grouped).map(
      ([entityType, entityTasks]) => () => this.processEntityBatch(entityType, entityTasks)
    ));
  }
  
  private async processEntityBatch(entityType: string, tasks: IndexingTask[]) {
    const entityIds = tasks.map(t => t.entityId);
    
    switch (entityType) {
      case 'pages':
        await this.indexPages(entityIds);
        break;
      case 'blocks':
        await this.indexBlocks(entityIds);
        break;
      case 'documents':
        await this.indexDocuments(entityIds);
        break;
    }
    
    // Mark tasks as completed
    await supabase
      .from('indexing_queue')
      .update({ status: 'completed', processed_at: new Date() })
      .in('id', tasks.map(t => t.id));
  }
}
```

4. Implement incremental index updates:
```typescript
// services/indexing/incremental-indexer.ts
export class IncrementalIndexer {
  async indexPages(pageIds: string[]) {
    // Fetch only changed content with checksums
    const pages = await supabase
      .from('pages')
      .select('id, title, content, content_checksum, updated_at')
      .in('id', pageIds);
    
    for (const page of pages.data) {
      // Check if content actually changed
      const existingIndex = await this.getExistingIndex(page.id);
      
      if (existingIndex?.content_checksum === page.content_checksum) {
        continue; // Skip if content unchanged
      }
      
      // Generate incremental updates
      const chunks = await this.generateIncrementalChunks(page, existingIndex);
      
      // Update only changed vectors
      await this.updateVectors(chunks);
    }
  }
  
  private async generateIncrementalChunks(page: Page, existingIndex?: IndexEntry) {
    // Smart diffing to identify changed sections
    const diff = this.computeContentDiff(existingIndex?.content, page.content);
    
    // Generate embeddings only for changed chunks
    return this.chunkAndEmbed(diff.changedSections);
  }
}
```

5. Create background re-indexing job system:
```typescript
// supabase/functions/background-reindex/index.ts
Deno.serve(async (req) => {
  const { workspaceId, fullReindex } = await req.json();
  
  // Schedule re-indexing job
  const job = await createReindexJob({
    workspaceId,
    type: fullReindex ? 'full' : 'incremental',
    priority: -1, // Low priority
    scheduledAt: new Date()
  });
  
  // Process in chunks to avoid timeouts
  const CHUNK_SIZE = 1000;
  let offset = 0;
  
  while (true) {
    const entities = await getEntitiesForReindex(workspaceId, offset, CHUNK_SIZE);
    
    if (entities.length === 0) break;
    
    // Queue for processing
    await queueEntitiesForIndexing(entities, job.id);
    
    offset += CHUNK_SIZE;
    
    // Yield to prevent Edge Function timeout
    await new Promise(resolve => setTimeout(resolve, 100));
  }
  
  return new Response(JSON.stringify({ jobId: job.id, status: 'scheduled' }));
});
```

6. Implement performance monitoring and optimization:
```typescript
// services/indexing/performance-monitor.ts
export class IndexingPerformanceMonitor {
  private metrics = {
    indexingLatency: new Map<string, number[]>(),
    batchSizes: [],
    errorRates: new Map<string, number>()
  };
  
  async monitorIndexingPipeline() {
    // Track indexing latency
    const latency = await this.measureIndexingLatency();
    
    // Auto-adjust batch sizes based on performance
    if (latency.p95 > 1000) { // If p95 > 1s
      await this.reduceBatchSize();
    } else if (latency.p95 < 200) { // If p95 < 200ms
      await this.increaseBatchSize();
    }
    
    // Monitor and alert on error rates
    const errorRate = await this.calculateErrorRate();
    if (errorRate > 0.05) { // 5% error threshold
      await this.triggerErrorAlert();
    }
  }
}
```

7. Create intelligent cache invalidation:
```typescript
// services/indexing/cache-invalidator.ts
export class IndexCacheInvalidator {
  async invalidateRelatedCaches(entityId: string, entityType: string) {
    // Invalidate direct entity cache
    await this.invalidateEntityCache(entityId);
    
    // Find and invalidate related searches
    const relatedSearches = await this.findRelatedSearches(entityId, entityType);
    
    for (const searchKey of relatedSearches) {
      await redis.del(`search:${searchKey}`);
    }
    
    // Invalidate workspace-level aggregations if needed
    if (entityType === 'pages') {
      await this.invalidateWorkspaceStats(entityId);
    }
  }
}
```

# Test Strategy:
1. Test PostgreSQL trigger functionality by creating, updating, and deleting pages/blocks, then verify entries appear in indexing_queue table with correct operation types and entity IDs within 50ms.

2. Verify Realtime subscription by monitoring indexing_queue changes and confirming the RealtimeIndexer receives events in real-time, buffers them correctly, and triggers batch processing after 500ms debounce period.

3. Test batch processing performance by queuing 10,000 indexing tasks across different entity types, then verify batch processor handles them with proper concurrency limits (5 concurrent), respects batch size (100), and completes all tasks without memory leaks.

4. Validate incremental indexing by modifying small sections of large documents (>10MB), then verify only changed chunks are re-indexed by checking embedding generation count and comparing content checksums.

5. Test background re-indexing by triggering full workspace re-index job for workspace with 50k+ entities, verify job processes in chunks without Edge Function timeouts, maintains low priority to not impact real-time operations, and completes successfully with progress tracking.

6. Performance test the entire pipeline by simulating 1000 concurrent users making rapid edits, verify p95 indexing latency < 1s, search results reflect changes within 2s, and system auto-adjusts batch sizes based on load.

7. Test error handling and recovery by simulating embedding service failures, database connection drops, and verify retry logic works correctly with exponential backoff, failed tasks are retried up to 3 times, and permanent failures are logged for manual intervention.

8. Verify cache invalidation by updating a page, then immediately searching for it and confirming fresh results are returned, related workspace statistics are updated, and no stale cache entries remain.

# Subtasks:
## 1. Create indexing queue database schema [done]
### Dependencies: None
### Description: Set up PostgreSQL tables for the indexing queue system including queue management and tracking
### Details:
Create the indexing_queue table with columns for entity tracking (id, entity_type, entity_id, operation, priority, retry_count, status, created_at, processed_at). Add indexes on status, created_at, and entity_id for efficient querying. Create enum types for operation and status fields.

## 2. Implement PostgreSQL triggers for content changes [done]
### Dependencies: 19.1
### Description: Create database triggers that automatically queue content changes for indexing
### Details:
Create the queue_content_for_indexing() function that inserts records into indexing_queue. Attach triggers to pages, blocks, and documents tables for INSERT, UPDATE, and DELETE operations. Handle both NEW and OLD row references correctly based on operation type.

## 3. Set up Supabase Realtime subscription service [done]
### Dependencies: 19.1
### Description: Implement RealtimeIndexer class to subscribe to indexing queue changes via Supabase Realtime
### Details:
Create RealtimeIndexer class in services/indexing/realtime-indexer.ts. Initialize Supabase channel subscription for 'indexing-updates'. Filter for pending status entries. Implement handleIndexingEvent method to process incoming events and buffer them for batch processing.

## 4. Implement debounced batch processing [done]
### Dependencies: 19.3
### Description: Create intelligent debouncing system to batch multiple indexing tasks efficiently
### Details:
Implement indexingBuffer Map to store pending tasks by entity ID. Create scheduleBatchProcessing method with 500ms debounce timer. Ensure newer tasks for same entity override older ones in buffer. Handle timer cancellation and rescheduling properly.

## 5. Create batch processor with concurrency control [done]
### Dependencies: 19.4
### Description: Build BatchIndexProcessor to handle bulk indexing operations with rate limiting
### Details:
Implement BatchIndexProcessor class with configurable BATCH_SIZE (100) and MAX_CONCURRENT (5). Create getTopPriorityTasks method to fetch highest priority pending tasks. Implement groupTasksByType for efficient entity grouping. Use p-limit library for concurrency control.

## 6. Implement entity-specific indexing methods [done]
### Dependencies: 19.5
### Description: Create specialized indexing logic for pages, blocks, and documents
### Details:
Implement processEntityBatch method with switch statement for entity types. Create indexPages, indexBlocks, and indexDocuments methods. Each method should fetch entity data, generate embeddings, and update vector store. Mark tasks as completed after successful processing.

## 7. Build incremental indexing system [done]
### Dependencies: 19.6
### Description: Implement IncrementalIndexer for efficient updates using content checksums
### Details:
Create IncrementalIndexer class with content checksum comparison. Implement getExistingIndex to fetch current index state. Create computeContentDiff method for smart diffing. Only generate embeddings for changed content sections to minimize API calls and processing time.

## 8. Create background re-indexing Edge Function [done]
### Dependencies: 19.7
### Description: Build Supabase Edge Function for scheduled full and incremental re-indexing
### Details:
Create background-reindex Edge Function that accepts workspaceId and fullReindex parameters. Implement chunked processing (1000 entities per chunk) to avoid timeouts. Add yield delays between chunks. Create job tracking for monitoring progress.

## 9. Implement performance monitoring system [done]
### Dependencies: 19.5, 19.6, 19.7
### Description: Build IndexingPerformanceMonitor to track and optimize indexing pipeline
### Details:
Create metrics tracking for indexing latency (p50, p95, p99), batch sizes, and error rates. Implement auto-adjustment of batch sizes based on latency metrics. Add alerting when error rate exceeds 5% threshold. Store metrics in time-series format for analysis.

## 10. Build intelligent cache invalidation [done]
### Dependencies: 19.6
### Description: Create IndexCacheInvalidator for smart cache management on content updates
### Details:
Implement invalidateEntityCache for direct entity caches. Create findRelatedSearches to identify affected search results. Build workspace-level aggregation invalidation for page changes. Use Redis for cache storage with appropriate TTLs.

## 11. Add error handling and retry mechanisms [done]
### Dependencies: 19.5, 19.6
### Description: Implement robust error handling with exponential backoff retry logic
### Details:
Add try-catch blocks in all processing methods. Implement exponential backoff for failed tasks (retry_count tracking). Create dead letter queue for tasks failing after max retries. Log detailed error information for debugging. Handle network failures gracefully.

## 12. Create monitoring dashboard and analytics [done]
### Dependencies: 19.9, 19.11
### Description: Build comprehensive monitoring for indexing pipeline health and performance
### Details:
Create dashboard showing real-time indexing queue depth, processing rate, error rates, and latency percentiles. Add historical trend analysis. Implement alerts for queue backlog, high error rates, or processing delays. Export metrics to observability platform.

