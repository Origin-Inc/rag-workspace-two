# Task ID: 33
# Title: Build AI-Powered Block Manipulation System
# Status: done
# Dependencies: 24
# Priority: medium
# Description: Implement a natural language command system that allows users to create, edit, transform, and manipulate blocks through conversational AI, enabling commands like 'Add a chart after this paragraph', 'Convert this list to a table', or 'Move this block above that one' with full context awareness and visual feedback.
# Details:
1. Create intelligent command parser using OpenAI GPT-4 with specialized prompt engineering:
```typescript
interface BlockCommand {
  action: 'create' | 'edit' | 'transform' | 'move' | 'merge' | 'split' | 'delete';
  targetBlocks: string[]; // Block IDs identified from context
  parameters: Record<string, any>;
  confidence: number;
}

const parseCommand = async (command: string, pageContext: PageContext) => {
  const completion = await openai.chat.completions.create({
    model: 'gpt-4',
    messages: [
      {
        role: 'system',
        content: `Parse block manipulation commands. Current page has ${pageContext.blocks.length} blocks.
        Available block types: ${pageContext.availableTypes.join(', ')}.
        User can reference blocks by: position (first, last, after, before), content ("the paragraph about X"), type ("the chart"), or ID.`
      },
      { role: 'user', content: command }
    ],
    functions: [blockManipulationSchema],
    function_call: { name: 'execute_block_command' }
  });
  return JSON.parse(completion.choices[0].message.function_call.arguments);
};
```

2. Implement block identification system with fuzzy matching and context awareness:
```typescript
class BlockIdentifier {
  identifyTargets(reference: string, blocks: Block[]): Block[] {
    // Handle positional references
    if (reference.match(/^(first|last|second|third)/i)) {
      return this.getByPosition(reference, blocks);
    }
    
    // Handle relative references
    if (reference.match(/(before|after|above|below)/i)) {
      return this.getRelativeBlocks(reference, blocks);
    }
    
    // Content-based search using embeddings
    if (reference.includes('about') || reference.includes('contains')) {
      return this.searchByContent(reference, blocks);
    }
    
    // Type-based identification
    if (reference.match(/(table|chart|list|paragraph|heading)/i)) {
      return this.getByType(reference, blocks);
    }
  }
}
```

3. Build command execution engine with transaction support and rollback:
```typescript
class BlockManipulator {
  async execute(command: BlockCommand): Promise<ExecutionResult> {
    const transaction = new BlockTransaction();
    
    try {
      switch (command.action) {
        case 'create':
          return await this.createBlock(command, transaction);
        case 'transform':
          return await this.transformBlock(command, transaction);
        case 'move':
          return await this.moveBlock(command, transaction);
        case 'merge':
          return await this.mergeBlocks(command, transaction);
        case 'split':
          return await this.splitBlock(command, transaction);
      }
      
      await transaction.commit();
      return { success: true, affectedBlocks: transaction.changes };
    } catch (error) {
      await transaction.rollback();
      throw error;
    }
  }
}
```

4. Implement intelligent block transformations with content preservation:
```typescript
class BlockTransformer {
  async transform(source: Block, targetType: BlockType): Promise<Block> {
    const transformers = {
      'list-to-table': this.listToTable,
      'paragraph-to-list': this.paragraphToList,
      'table-to-chart': this.tableToChart,
      'markdown-to-blocks': this.markdownToBlocks
    };
    
    const key = `${source.type}-to-${targetType}`;
    if (transformers[key]) {
      return await transformers[key](source);
    }
    
    // Use AI for complex transformations
    return await this.aiTransform(source, targetType);
  }
  
  private async aiTransform(source: Block, targetType: BlockType): Promise<Block> {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        {
          role: 'system',
          content: `Transform block content from ${source.type} to ${targetType}. Preserve all information.`
        },
        { role: 'user', content: JSON.stringify(source.content) }
      ]
    });
    return this.parseTransformation(completion.choices[0].message.content, targetType);
  }
}
```

5. Create visual feedback system with preview and animations:
```typescript
class BlockManipulationUI {
  async showPreview(command: BlockCommand): Promise<boolean> {
    const preview = document.createElement('div');
    preview.className = 'block-manipulation-preview';
    
    // Generate visual preview of changes
    const changes = await this.generatePreview(command);
    
    // Show ghost blocks for new content
    if (command.action === 'create') {
      this.showGhostBlock(changes.position, changes.content);
    }
    
    // Highlight affected blocks
    changes.affected.forEach(blockId => {
      this.highlightBlock(blockId, 'will-change');
    });
    
    // Show confirmation dialog
    return await this.confirmDialog({
      title: `${command.action} Block`,
      preview: changes,
      message: `This will ${command.action} ${changes.affected.length} block(s)`
    });
  }
  
  animateExecution(result: ExecutionResult): void {
    // Smooth transitions for moves
    if (result.type === 'move') {
      this.animateMove(result.from, result.to);
    }
    
    // Fade in for new blocks
    if (result.type === 'create') {
      this.fadeIn(result.newBlockId);
    }
    
    // Morph animation for transformations
    if (result.type === 'transform') {
      this.morphBlock(result.blockId, result.newContent);
    }
  }
}
```

6. Implement comprehensive undo/redo system with command history:
```typescript
class CommandHistory {
  private history: BlockCommand[] = [];
  private pointer: number = -1;
  private snapshots: Map<number, PageSnapshot> = new Map();
  
  async execute(command: BlockCommand): Promise<void> {
    // Take snapshot before execution
    const snapshot = await this.takeSnapshot();
    
    // Execute command
    const result = await this.manipulator.execute(command);
    
    // Store in history
    this.history = this.history.slice(0, this.pointer + 1);
    this.history.push(command);
    this.snapshots.set(++this.pointer, snapshot);
    
    // Limit history size
    if (this.history.length > 100) {
      this.history.shift();
      this.snapshots.delete(this.pointer - 100);
    }
  }
  
  async undo(): Promise<void> {
    if (this.pointer >= 0) {
      const snapshot = this.snapshots.get(this.pointer--);
      await this.restoreSnapshot(snapshot);
    }
  }
}
```

7. Add natural language feedback and error handling:
```typescript
class AIFeedback {
  async explainAction(command: BlockCommand, result: ExecutionResult): Promise<string> {
    if (result.success) {
      return this.generateSuccessMessage(command, result);
    }
    
    // Explain why command failed
    const completion = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        {
          role: 'system',
          content: 'Explain in simple terms why the block manipulation failed and suggest alternatives.'
        },
        {
          role: 'user',
          content: JSON.stringify({ command, error: result.error })
        }
      ]
    });
    
    return completion.choices[0].message.content;
  }
}
```

8. Integrate with existing LLM orchestration and page context:
```typescript
class BlockManipulationIntegration {
  constructor(
    private llmOrchestrator: LLMOrchestrationService,
    private pageContext: PageContextService,
    private blockService: DatabaseBlockService
  ) {}
  
  async processNaturalLanguageCommand(input: string): Promise<void> {
    // Get current page context
    const context = await this.pageContext.getCurrentContext();
    
    // Parse command with LLM orchestration
    const command = await this.llmOrchestrator.parseBlockCommand(input, context);
    
    // Show preview with confidence score
    if (command.confidence < 0.7) {
      const clarification = await this.requestClarification(command);
      if (!clarification.confirmed) return;
    }
    
    // Execute with visual feedback
    const ui = new BlockManipulationUI();
    if (await ui.showPreview(command)) {
      const result = await this.manipulator.execute(command);
      ui.animateExecution(result);
      
      // Store in command history
      await this.history.execute(command);
    }
  }
}
```

# Test Strategy:
1. Test natural language parsing with 50+ diverse commands including 'Add a table after the second paragraph', 'Move the chart above the introduction', 'Convert this bullet list to a numbered list', verifying 90%+ accuracy in command interpretation and target block identification.

2. Verify block identification correctly handles positional references (first, last, third), relative references (before/after/above/below), content-based references ('the paragraph about pricing'), and type-based references ('all tables', 'the chart').

3. Test complex transformations preserve all content when converting between types: list→table, table→chart, paragraph→list, markdown→blocks, ensuring no data loss and proper formatting.

4. Verify preview system shows accurate visual representation of changes before execution, with ghost blocks for additions, highlights for modifications, and animation previews for moves.

5. Test undo/redo maintains complete history for last 100 operations, correctly restores page state including block positions, content, and metadata.

6. Load test with rapid command execution (10 commands/second) to verify transaction integrity and no race conditions in concurrent block manipulations.

7. Test error handling provides helpful natural language feedback when commands fail, suggesting alternatives and explaining why the operation couldn't be completed.

8. Verify integration with existing LLM orchestration service correctly passes page context and receives structured commands.

9. Test accessibility with screen readers, ensuring all visual feedback has appropriate ARIA labels and announcements.

10. Measure performance: command parsing < 500ms, preview generation < 200ms, execution with animations < 300ms for typical operations.

# Subtasks:
## 1. Create Natural Language Command Parser [done]
### Dependencies: None
### Description: Implement AI-powered command parser that interprets natural language block manipulation requests using GPT-4
### Details:
Build a command parser that can understand requests like 'Add a chart after this paragraph' and convert them to structured BlockCommand objects with action types, target blocks, and parameters. Include confidence scoring.

## 2. Build Block Identification System [done]
### Dependencies: None
### Description: Create a system that can identify blocks by position, content, type, or relative references
### Details:
Implement BlockIdentifier class with methods like getByPosition (first, last, second), getRelativeBlocks (before/after/above/below), searchByContent (using embeddings), and getByType. Support fuzzy matching and context awareness.

## 3. Implement Block Manipulation Engine [done]
### Dependencies: None
### Description: Create BlockManipulator class with transaction support for executing block operations
### Details:
Build execution engine that handles create, transform, move, merge, split, and delete operations. Include transaction support with rollback capability, proper error handling, and integration with the existing CommandManager for undo/redo.

## 4. Create Block Transformation System [done]
### Dependencies: None
### Description: Build intelligent transformations between block types with content preservation
### Details:
Implement BlockTransformer class with specific transformations: list-to-table, table-to-chart, paragraph-to-list, markdown-to-blocks. Use AI for complex transformations that don't have predefined rules. Ensure all content is preserved during transformations.

## 5. Build Visual Feedback and Preview System [done]
### Dependencies: None
### Description: Create UI components for showing previews, ghost blocks, and animations
### Details:
Implement BlockManipulationUI class with showPreview method for visual representation of changes, ghost blocks for new content, highlights for affected blocks, and confirmation dialogs with confidence scores. Include smooth animations for moves, fades, and morphing transformations.

## 6. Implement AI-Powered Chart Creation [done]
### Dependencies: None
### Description: Enable creation of charts from natural language commands and data extraction
### Details:
Build chart generation system that can: 1) Parse chart creation commands like 'create a bar chart of sales data', 2) Extract data from text/tables automatically, 3) Determine appropriate chart types (bar, line, pie, scatter), 4) Generate chart configuration with proper labels and formatting, 5) Support data visualization libraries like Chart.js or D3.js.

## 7. Create Natural Language Feedback System [done]
### Dependencies: None
### Description: Build AI feedback system for explaining actions and handling errors
### Details:
Implement AIFeedback class that provides: 1) Success messages explaining what was done, 2) Error explanations in simple terms when commands fail, 3) Alternative suggestions when operations can't be completed, 4) Clarification requests when confidence is low, 5) Progress updates for long-running operations.

## 8. Integrate with Existing Editor and Services [done]
### Dependencies: None
### Description: Connect the block manipulation system with current editor components and services
### Details:
Create BlockManipulationIntegration class to: 1) Connect with existing LLMOrchestrationService, 2) Integrate with PageContextService for current page state, 3) Use DatabaseBlockService for block operations, 4) Hook into existing CommandManager for undo/redo, 5) Update TiptapEditor and BlockEditor components to support new commands.

## 9. Add Command Input UI Component [done]
### Dependencies: None
### Description: Create a command bar interface for entering natural language block commands
### Details:
Build a command input component with: 1) Floating command bar that can be triggered with hotkey (Cmd+K), 2) Auto-complete suggestions based on command history, 3) Real-time syntax highlighting for recognized commands, 4) Command history dropdown, 5) Voice input support option, 6) Integration with the command preview system.

## 10. Write Comprehensive Tests and Documentation [done]
### Dependencies: None
### Description: Create test suite and documentation for the block manipulation system
### Details:
Implement tests for: 1) Natural language parsing accuracy (50+ test commands), 2) Block identification edge cases, 3) Transformation data preservation, 4) Animation performance benchmarks, 5) Undo/redo transaction integrity, 6) Error handling scenarios. Create documentation with examples of supported commands, API reference, and integration guide.

## 11. Implement Contextual AI Assistant for Individual Blocks [done]
### Dependencies: None
### Description: Add AI invocation capability directly on each block through toolbars, context menus, and keyboard shortcuts
### Details:
Create a contextual AI assistant system that allows users to invoke AI on specific blocks without describing their location. Implementation includes:

1. BlockAIAssistant class with type-specific suggestions:
   - Text blocks: 'Shorten', 'Make formal', 'Translate', 'Add bullet points'
   - Database blocks: 'Add column', 'Filter empty rows', 'Generate statistics', 'Create chart from data'
   - Chart blocks: 'Change type', 'Update colors', 'Add trendline'
   - Code blocks: 'Add comments', 'Refactor', 'Add error handling'

2. Multiple entry points for AI invocation:
   - AI button (✨) in block toolbars
   - Right-click context menu option
   - Keyboard shortcut (Cmd+Shift+A when block focused)
   - Three-dot menu integration

3. AIContextPanel component:
   - Quick actions based on block type
   - Custom command input field
   - Recent commands for this block type
   - Inline preview of changes

4. Block context passing:
   - Automatic blockId, blockType, blockContent inclusion
   - No need for users to describe which block
   - Sibling blocks and page context awareness

5. Integration with existing systems:
   - Connect with BlockManipulator from subtask 33.3
   - Use command parser from subtask 33.1
   - Show previews using system from subtask 33.5
   - Store in command history from subtask 33.9

Example implementation:
```typescript
interface BlockAIContext {
  blockId: string;
  blockType: BlockType;
  blockContent: any;
  position: BlockPosition;
  onCommand: (command: string) => void;
}

<BlockContainer>
  <BlockToolbar>
    <AIButton onClick={() => openAIAssistant(blockContext)} />
  </BlockToolbar>
  {showAI && <AIContextPanel context={blockContext} />}
</BlockContainer>
```

This provides intuitive, context-aware AI assistance exactly where users need it, complementing the global command bar from subtask 33.9.

